# 加州房价回归实验

本项目是一个基于加州房价数据集的完整机器学习回归实验，包含数据预处理、模型构建、训练优化和结果分析等全流程。

## 📋 项目概述

### 实验目标
基于加州房价数据集(California Housing Prices)，通过20,640条记录的多维属性(8个数值特征 + 1个分类特征)，预测房屋售价（目标变量 median_house_value），构建高精度的回归模型。

## 📊 数据集详情

### 特征说明
| 特征名 | 类型 | 说明 | （9个指标对房价都有潜在影响）
|--------|------|------|
| longitude | 数值 | 经度 |（地理位置，如沿海区域房价更高）
| latitude | 数值 | 纬度 |（同上，可结合聚类生成 “区域” 特征）
| housing_median_age | 数值 | 房屋中位年龄 |（老房 / 新房对价格的影响，可能非线性）
| total_rooms | 数值 | 区域总房间数 |（结合家庭数得到 “人均房间数”）
| total_bedrooms | 数值 | 区域总卧室数 (有缺失值) |（要先处理缺失值）
| population | 数值 | 区域人口数 |（人口密度与房价的负相关 / 正相关关系需验证）
| households | 数值 | 区域家庭数 |（衍生 “每户人口”“每户房间数” 等关键特征）
| median_income | 数值 | 居民中位收入 |（核心因素，收入越高房价通常越高，线性相关性强）
| ocean_proximity | 分类 | 距离海洋位置 |（5个离散的类别，需编码为数值特征）
| median_house_value | 数值 | 房屋中位价值 (目标变量) |

### 统计信息
- 总样本数: 20,640
- 缺失值: total_bedrooms字段有207个缺失值（csv中为空格）
- 分类特征: ocean_proximity (5个类别)（包括<1H OCEAN, INLAND, ISLAND, NEAR BAY,   NEAR OCEAN）
- 数值特征范围: 经度(-124.35~-114.31), 纬度(32.54~41.95)

### 实验阶段
1. **数据准备**: 数据加载、探索性分析
2. **数据预处理**: 缺失值处理（total_bedrooms）、数值特征编码（ocean_proximity）、特征工程（如衍生 “每户房间数”“每户人口”等，）。
3. **模型搭建**: 建立多种回归模型
4. **模型训练**: 训练和初步评估
5. **模型优化**: 超参数调优、模型集成
6. **模型检测**: 最终评估和性能分析
7. **总结**: 实验报告和经验总结

## 🛠️ 环境配置
# 1. 创建虚拟环境 
conda create -n ML-1 python=3.10.18
conda activate ml_lab
# 2. 安装依赖
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

## 📁 项目结构

lab1-Regression-submit/
├── data/                          # 数据存储
│   ├── housing_processed.csv      # 预处理后的数据
│   └── housing.csv                # 原始数据集
├── src/                           # 源代码
│   ├── utils/                     # 工具函数
│   │   ├── data_loader.py        # 数据加载
│   │   ├── feature_engineering.py # 特征工程
│   │   ├── visualization.py      # 可视化
│   │   └── metrics.py            # 评估指标
│   ├── data_preparation.py      # 数据准备阶段
│   ├── data_preprocessing.py     # 数据预处理阶段
│   ├── model_building.py        # 模型搭建阶段
│   ├── model_training.py        # 模型训练阶段
│   ├── model_optimization.py   # 模型优化阶段
│   └── model_evaluation.py     # 模型检测阶段
├── models/                        # 模型存储
│   ├── baseline_models/           # 基础模型
│   ├── optimized_models/          # 优化模型
│   └── model_comparison_results/  # 模型对比结果
├── results/                       # 实验结果
│   ├── plots/                     # 可视化图表
│   ├── metrics/                   # 评估指标
│   └── reports/                   # 分析报告
├── main.py                        # 主程序
├── requirements.txt               # 依赖列表
├── project.md                     # 回归实验操作步骤
└── README.md                      # 详细实验规划


## 🛠️ 技术栈

- **编程语言**: Python 3.9
- **数据处理**: Pandas（处理表格的缺失值，重复值和类型转换，进行按条件选行、按标签/位置选数据，分组以及数据合并等操作）， NumPy（快速处理多维数值数组的数学运算）
- **可视化**: Matplotlib（在figure画布，axes子图上手动设置细节，常绘制散点图，柱状图和分布直方图）, Seaborn（封装更高级的函数，自动处理分组，配色更美观）
- **机器学习模型构建**: Scikit-learn（提供数据预处理，模型选择，训练，评估和预测等功能，封装好，并提供统一接口的机器学习工具库）
- **开发工具**: Jupyter Notebook, VS Code

## 🔍 关键概念
- **统一接口（fit→predict→score）**是指 Scikit-learn 中所有机器学习模型（无论是分类、回归、聚类还是其他任务）都遵循相同的核心方法命名和使用逻辑，目的是降低学习成本，让用户能用一致的流程操作不同算法。
- **fit(X, y)模型训练**让模型 “学习” 数据中的规律。模型会根据输入的训练数据（特征X和目标y）调整内部参数（比如线性回归的系数、决策树的分裂阈值等）。输入：
X：训练特征数据（通常是二维数组 / 矩阵，形状为[样本数, 特征数]）；
y：训练目标变量（一维数组，形状为[样本数]，分类任务中是类别标签，回归任务中是连续值）。
- **predict(X)模型预测**应用训练好的模型已通过fit学到参数）对新数据（特征X）进行预测。
输入：新的特征数据X（形状与fit时的X一致，即[新样本数, 特征数]）。
输出：预测的目标值（一维数组，分类任务中是预测的类别，回归任务中是预测的连续值）。
- **score(X, y)：模型评估**  衡量预测效果，快速计算模型在给定数据（X和真实目标y）上的表现好坏，返回一个评估指标（数值越大通常表示效果越好）。
输入：特征X和对应的真实目标y（可以是训练集或测试集）。
输出：评估指标（具体指标因任务而异）：
分类任务：默认返回准确率（accuracy）（正确预测的样本占比）；
回归任务：默认返回R² 分数（衡量模型解释目标变量变异的程度，范围 0~1，越接近 1 越好）。

## 🎯 实验要求

### 基本要求
- ✅ 自行选取数据集进行回归预测
- ✅ 完整的实验流程（实验按 "数据准备→预处理→模型搭建→训练→优化→检测→总结"7 个阶段展开）
- ✅ 可视化展示结果
- ✅ 模型性能对比分析（借助MSE，MAE，RMSE，R²等指标）

### 加分内容
- 🌟 手写实现回归模型而不是调库
- 🌟 对调参（学习率和正则化参数）进行深入讨论
- 🌟 探究输入变量参数之间的关联性
- 🌟 数据升维、数据降维技术
- 🌟 高级特征工程


## 📈 预期成果

### 模型性能目标
- **R²**: >0.8  意味着模型能解释 80% 以上的房价变异。
- **RMSE**: <50,000 （预测 20 万的房子，误差在 ±5 万内，无“预测20万实际50万” 的极端偏差）。
- **MAE**: <35,000   MAE 对异常值不敏感，通常小于 RMSE（放大极端误差），3.5w能保证模型的 “稳健性”，避免极端误差对整体评估的干扰，目标设置合理。

### 实验产出
- 多个训练好的回归模型（并选出SOTA）
- 完整的性能评估报告
- 特征重要性分析
- 可视化图表和结果
- 实验总结报告

### 探索性数据分析（Exploratory Data Analysis, EDA） 
在数据准备阶段，探索性数据分析（Exploratory Data Analysis, EDA） 是核心环节，其核心目标是 **“全面理解数据、发现隐藏问题、挖掘潜在规律”**，为后续的特征工程、模型选择提供依据。EDA 并非单一任务，而是一套系统性的分析流程，通常涵盖 5 大核心工作模块，每个模块均围绕 “数据认知→问题定位→业务关联” 展开，具体如下：
1. 回答 3 个问题：“数据有多大？包含什么？格式对不对？”
区分特征类型（分类型，数值型包括连续性和离散型），对数值型特征计算 “集中趋势” 和 “离散程度” 指标，快速定位数据的 “正常范围”（输出均值、中位数、标准差、最值、四分位数）
### 均值和中位数若差异大，可能存在偏态分布（如 “收入” 均值远大于中位数，说明有高收入极端值）。
标准差越大，数据的离散程度越高；最大/小值可以快速发现明显异常。
2. EDA需要系统性检测 原始数据存在的缺失、重复、异常、不一致等缺陷。
*** 数据一致性校验 ***检查特征间的逻辑矛盾（基于业务规则），避免 “数据自相矛盾”。
常用场景：
时间逻辑：“成交日期” 不能早于 “建成日期”（否则房龄为负）。
数值逻辑：“总面积” 不能小于 “使用面积”（否则面积差为负）。
分类逻辑：“学历” 为 “小学” 的样本，“毕业年限” 不能小于 6 年（小学学制 6 年）。
目的：修正 “逻辑错误数据”（如将 “成交日期 < 建成日期” 的样本修正为正确日期，或删除无法修正的错误样本）。
示例：发现 10 条样本 “总面积 = 80㎡，使用面积 = 90㎡”，确认是录入颠倒后，交换两者数值。
3. 单变量分析：聚焦于 **“每个特征自身的分布特征和业务含义”**，包括：
- 数值型特征：计算均值、中位数、标准差、最小值、最大值、四分位数等统计指标，可视化展示分布（如直方图、箱线图），分析异常值、缺失值、异常分布等问题。
- 分类型特征：统计每个类别出现的次数（如性别、教育水平、房屋类型等），可视化展示类别分布（如柱状图、饼图），分析类别不平衡问题。
常用工具：
直方图（matplotlib.hist()）：看整体分布趋势。
Q-Q 图（scipy.stats.probplot()）：验证是否符合正态分布（点越贴近直线，越接近正态）。
核密度图（seaborn.kdeplot()）：更平滑地展示分布峰值。
条形图（seaborn.countplot()）：展示各类别样本数。
饼图（matplotlib.pie()）：展示各类别占比。
典型分布与业务解读：
正态分布：如 “成年人身高”，大部分集中在均值附近，符合自然规律。
右偏分布：如 “用户消费额”，大部分用户消费低，少数高消费用户拉长尾 —— 业务上对应 “头部用户”。
双峰分布：如 “城市通勤时间”，出现两个峰值（30 分钟、60 分钟）—— 可能对应 “近郊区” 和 “远郊区” 用户。
示例：“房龄” 特征的直方图显示为右偏分布，峰值在 5-10 年（次新房），长尾在 20 年以上（老房）—— 业务上说明市场以次新房为主，老房占比低。
4. 多变量分析：聚焦于 **“特征与特征、特征与目标变量的关系”**目标变量是回归 / 分类任务的预测对象（如房价预测中的 “房价”，客户流失预测中的 “是否流失”），EDA 需优先分析 “哪些特征对目标变量影响大”。检测特征间的 “冗余关联”（如多重共线性）或 “互补关联”（如可组合成新特征），删除或合并。
### housing.csv进行EDA后的可能结果：
  数据形状: (20640, 10)
  列名: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 
  'total_bedrooms', 'population', 'households', 'median_income', 
  'median_house_value', 'ocean_proximity']

  缺失值检查:
  total_bedrooms    207

  目标变量分析:
  最小值: 14999.00
  最大值: 500001.00
  均值: 206855.82
  中位数: 179700.00

  分类变量分析:
  <1H OCEAN     9136
  INLAND        6551
  NEAR OCEAN    2658
  NEAR BAY      2290
  ISLAND           5


### 正则化的重要性
- 普通线性回归的目的是基于已有数据对未来数据进行预测，Ridge/Lasso/Elastic Net的目的是在普通线性回归的基础上加入正则化项，防止过度拟合训练数据中的噪声或偶然模式（尤其是数据量少、特征维度高时，输入稍有变动就会导致预测值剧烈变化），导致参数异常增大，最终表现为模型 “记住” 了训练数据的细节而非规律 ，在新数据上表现很差。
- 惩罚大参数（参数的平方和/绝对值之和越大，正则化项越大，惩罚越重，限制模型的复杂度，强制模型参数保持较小的值，避免对局部噪声过度敏感；
- 正则化的目的是让模型更 “简单”（如 Lasso 直接剔除冗余特征），从而更倾向于学习数据中的整体规律而非局部细节；通过合适的正则化参数，避免过拟合和欠拟合，最终提升模型的泛化能力，让训练好的模型在未见过的新数据上依然能稳定预测。

### 模型评价
- 首先，可以尝试使用 LogisticRegression() ，如果测试 RMSE 的值较低，而训练 RMSE 的值较高，则模型可能过度拟合。 
- 然后，可以尝试应用每种类型的正则化并查看输出。 还可以为超参数 alpha 和 l1_ratio 尝试不同的有效值。
- 最后，通过查看训练集和测试集上的 RMSE 来选择一个好的模型。 一个好的模型既不会过拟合也不会欠拟合数据。 它应该能够在训练数据上表现良好，并且在看不见的数据（测试数据）上也能很好地泛化。

### k 折交叉验证
- 在回归实验中，k 折交叉验证的核心价值就在于解决 “单次训练 - 测试划分” 的局限性。主要目的不是直接选择模型，而是更精准地评估单个模型的泛化能力，但它可以作为模型选择的重要依据。
- 如果只将数据分成 1 次训练集和测试集，结果会受划分方式影响（比如恰好测试集都是容易预测的数据）。k 折通过将数据分成 k 份，轮流用 k-1 份训练、1 份测试，能得到 k 个误差结果，最终取平均，减少随机性。
- 在小数据集上，单次划分会浪费部分数据（测试集无法参与训练）。k 折让几乎所有数据都参与过训练和测试，提升数据利用率，让评估结果更可靠。
- 当有多个候选模型（比如不同参数的线性回归、不同深度的决策树）时，分别计算每个候选模型的平均误差（如 MSE、MAE），k 折交叉验证可以间接帮助选择效果最好的模型。

### 特征工程
- 在回归模型（如线性回归、XGBoost 回归、神经网络回归等）中，特征工程的核心目标是将原始数据转化为 “让模型能有效学习” 的特征，最终提升模型的预测精度、稳定性和可解释性。
- 通过特征工程，可以挖掘数据中的潜在信息，揭示变量间的复杂关系，帮助模型更好地捕捉目标变量的变化规律。具体步骤如下：
1. **数据清洗**: 处理缺失值（特征缺失会导致模型无法训练或者精度偏差过大，均值/中位数分组填充或者众数和特征值标记是常用的方法）、异常值（线性回归和神经网络都对异常值很敏感，会拉高/低预测结果）和重复值，对分类变量进行编码，确保数据质量，方便模型学习。
2. **特征转换**: 将数据集拆分为输入特征后，回归模型通常仅接受数值型特征，且部分模型（如线性回归）难以捕捉非线性关系，需对特征类型和分布进行行归一化(min-max)、标准化（z-score）。
3. **特征衍生**: 原始特征往往不足以体现与目标变量的关联，需要结合实际场景、基于现有特征创建新特征（如 “每户房间数”“每户人口”等），增强模型的表达能力。
4. **特征选择**: 通过相关性分析、递归特征消除等方法，剔除冗余或无关特征，减少噪声，提高模型泛化能力。目标是筛选出 “对目标变量最有预测力” 的特征子集。
5. **特征降维**: 特征缩放，消除量纲的影响；删除与其他特征高度相关的冗余特征（如删除 “使用面积”，保留 “总面积”）；用主成分分析（PCA）将高相关特征合并为少数 “不相关的主成分”（缺点是可解释性降低）；正则化：用岭回归（Ridge，L2 正则化）缓解共线性（不删除特征，而是缩小系数绝对值），减少计算复杂度，防止过拟合。

### 异常值处理方法 （如用 3σ 法则剔除）
1. *** 箱线图（IQR）法：基于分位数的鲁棒检测***：通过数据的四分位数范围（Interquartile Range）定义异常值，不受极端值影响，对偏态分布数据更友好（如收入、房价等右偏数据）。
2. ***Z-score 法：基于正态分布的偏离度检测***：衡量单个数据点与均值的偏离程度（以标准差为单位），假设数据近似正态分布（如身高、体重等自然数据），对极端值较敏感（因均值和标准差易受异常值影响）。
3. ***DBSCAN 聚类法：基于密度的离群点检测***：通过 “数据点周围的密度” 判断异常：密集区域内的点为正常样本，孤立点（密度极低）为异常值，适用于高维数据或非线性分布数据（如用户行为特征、多维传感器数据）。

### 特征转换方法
1. 分类型特征（如 “性别”“城市”）无法直接输入模型，需转为数值。
编码方法	               适用场景	                       示例
One-Hot 编码    低基数标称变量（无顺序，类别数 < 10）    性别（男1女0）户型（一居100）
标签编码          有序序数变量（有大小关系）    学历（小学 = 1，中学 = 2，大学 = 3）
目标编码         高基数分类变量（类别数 > 50）    城市（“城市的平均房价” 作为编码值）
2. 处理偏态分布（如 “收入”“用户消费额” 常呈右偏分布），使特征更接近正态分布（符合线性回归的 “正态性假设”）。
常用方法：对数转换（log(x+1)，避免 x=0）、平方根转换（sqrt(x)）、Box-Cox 转换。
3. 分箱（离散化）：将连续数值转为离散区间，核心作用是捕捉非线性关系（如 “年龄与收入” 并非线性，分箱后更易体现 “30-45 岁收入最高” 的规律）、处理异常值。
常用方法：
等宽分箱：区间宽度相同（如年龄 0-20、21-40、41-60），易受异常值影响。
等频分箱：每个区间样本数相同（如将 “房价” 分为 5 箱，每箱含 20% 样本），更鲁棒。
聚类分箱：用 K-means 将相似数值聚为一类（如将 “贷款金额” 聚为 3 箱：低额、中额、高额）。

### 常见特征衍生
构造类型	          逻辑	                         示例（房价预测任务）
时间特征	从日期提取周期 / 时段信息	成交日期→“月份”“是否旺季”“房龄（当前年 - 建成年）”
统计特征	基于分组 / 窗口的统计量	           区域→“同区域近 6 个月成交均价”
交互特征	特征间的组合（捕捉协同效应）	面积 × 户型→“每居室面积（总面积 / 户型居室数）”
比例 / 比率特征	   体现相对关系	             总价 × 面积→“单位面积房价（总价 / 总面积）”
计数特征	高频行为 / 属性的计数	        小区→“小区内配套设施数（超市 / 学校 / 医院）

### 特征选择的方法
方法类型	               核心逻辑	常用工具 / 指标
过滤法（Filter）	基于特征自身统计属性筛选，不依赖模型	皮尔逊相关系数（衡量数值特征与目标的线性相关）、互信息（衡量任意类型特征的关联，含非线性）、方差阈值（剔除方差极小的常量特征）
包裹法（Wrapper）	以模型性能为准则，筛选特征子集	递归特征消除（RFE）：用线性回归为基模型，逐步剔除系数最小的特征，直到模型 R² 最优
嵌入法（Embedded）	特征选择与模型训练同步进行	L1 正则化（Lasso 回归，系数为 0 的特征直接剔除）、树模型特征重要性（XGBoost/RF 的feature_importances_）

### 特征缩放方法
缩放方法	                    公式	                         适用场景
标准化（Z-score）	(x-μ)/σ（μ 为均值，σ 为标准差）	特征近似正态分布、模型对异常值不敏感
归一化（Min-Max）	(x-min)/(max-min)（缩至 [0,1]）	特征有明确边界（如面积）、固定输出范围
鲁棒缩放（RobustScaler）	(x - 中位数)/IQR          特征含较多异常值（如 “用户收入”）