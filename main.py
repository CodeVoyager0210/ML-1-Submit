#!/usr/bin/env python3
"""
åŠ å·æˆ¿ä»·å›å½’å®éªŒä¸»ç¨‹åº

æœ¬ç¨‹åºæ˜¯åŠ å·æˆ¿ä»·å›å½’å®éªŒçš„å…¥å£ç‚¹ï¼Œè´Ÿè´£æŒ‰é¡ºåºæ‰§è¡Œå„ä¸ªå®éªŒé˜¶æ®µï¼š
1. æ•°æ®å‡†å¤‡ (Data Preparation)
2. æ•°æ®é¢„å¤„ç† (Data Preprocessing)
3. æ¨¡å‹æ­å»º (Model Building)
4. æ¨¡å‹è®­ç»ƒ (Model Training)
5. æ¨¡å‹ä¼˜åŒ– (Model Optimization)
6. æ¨¡å‹è¯„ä¼° (Model Evaluation)

ä½œè€…: Claude
æ—¥æœŸ: 2025-01-01
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Optional

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.utils.data_loader import HousingDataLoader, quick_data_overview
from src.utils.visualization import HousingDataVisualizer
from src.data_preparation import DataPreparationPipeline


def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None) -> None:
    """
    è®¾ç½®æ—¥å¿—é…ç½®

    Args:
        log_level (str): æ—¥å¿—çº§åˆ«
        log_file (Optional[str]): æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    # åˆ›å»ºæ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # é…ç½®æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(getattr(logging, log_level.upper()))

    # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)

    # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)


def parse_arguments() -> argparse.Namespace:
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°

    Returns:
        argparse.Namespace: è§£æåçš„å‚æ•°
    """
    parser = argparse.ArgumentParser(
        description="åŠ å·æˆ¿ä»·å›å½’å®éªŒä¸»ç¨‹åº",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è¿è¡Œå®Œæ•´çš„æ•°æ®å‡†å¤‡é˜¶æ®µ
  python main.py --phase preparation --data-dir data --output-dir results

  # ä»…è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
  python main.py --phase preparation --data-dir data --output-dir results --skip-plots

  # è¿è¡Œæ‰€æœ‰é˜¶æ®µï¼ˆå®Œæ•´å®éªŒï¼‰
  python main.py --phase all --data-dir data --output-dir results

  # ä½¿ç”¨è‡ªå®šä¹‰kå€¼è¿›è¡Œäº¤å‰éªŒè¯
  python main.py --phase all --data-dir data --output-dir results --k-folds 10

  # å¯ç”¨è¯¦ç»†æ—¥å¿—
  python main.py --phase preparation --data-dir data --output-dir results --log-level DEBUG
        """
    )

    parser.add_argument(
        "--phase",
        type=str,
        choices=["preparation", "preprocessing", "building", "training", "optimization", "evaluation", "all"],
        default="preparation",
        help="è¦è¿è¡Œçš„å®éªŒé˜¶æ®µ (é»˜è®¤: preparation)"
    )

    parser.add_argument(
        "--data-dir",
        type=str,
        default="data",
        help="æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: data)"
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="results",
        help="è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: results)"
    )

    parser.add_argument(
        "--k-folds",
        type=int,
        default=5,
        help="KæŠ˜äº¤å‰éªŒè¯çš„æŠ˜æ•° (é»˜è®¤: 5)"
    )

    parser.add_argument(
        "--log-level",
        type=str,
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)"
    )

    parser.add_argument(
        "--log-file",
        type=str,
        default=None,
        help="æ—¥å¿—æ–‡ä»¶è·¯å¾„ (å¯é€‰)"
    )

    parser.add_argument(
        "--skip-plots",
        action="store_true",
        help="è·³è¿‡å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ"
    )

    parser.add_argument(
        "--force-overwrite",
        action="store_true",
        help="å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶"
    )

    parser.add_argument(
        "--test-mode",
        action="store_true",
        help="æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨å°‘é‡æ•°æ®è¿›è¡Œå¿«é€ŸéªŒè¯"
    )

    return parser.parse_args()


def run_data_preparation(args: argparse.Namespace) -> None:
    """
    è¿è¡Œæ•°æ®å‡†å¤‡é˜¶æ®µ

    Args:
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°
    """
    logger = logging.getLogger(__name__)
    logger.info("=" * 60)
    logger.info("å¼€å§‹é˜¶æ®µä¸€ï¼šæ•°æ®å‡†å¤‡ (Data Preparation)")
    logger.info("=" * 60)

    try:
        # åˆ›å»ºæ•°æ®å‡†å¤‡ç®¡é“
        pipeline = DataPreparationPipeline(
            data_dir=args.data_dir,
            output_dir=args.output_dir
        )

        # æ­¥éª¤1ï¼šåŠ è½½å’ŒéªŒè¯æ•°æ®
        logger.info("æ­¥éª¤ 1/3: åŠ è½½å’ŒéªŒè¯æ•°æ®")
        if args.test_mode:
            # æµ‹è¯•æ¨¡å¼ï¼šåªåŠ è½½å‰1000è¡Œæ•°æ®
            pipeline.data_loader.data = pipeline.data_loader.load_raw_data().head(1000)
            logger.info("æµ‹è¯•æ¨¡å¼ï¼šä»…ä½¿ç”¨å‰1000è¡Œæ•°æ®")
        else:
            pipeline.load_and_validate_data()

        # æ­¥éª¤2ï¼šæ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆé™¤éè·³è¿‡å›¾è¡¨ï¼‰
        if not args.skip_plots:
            logger.info("æ­¥éª¤ 2/3: æ¢ç´¢æ€§æ•°æ®åˆ†æ")
            pipeline.perform_eda()
        else:
            logger.info("æ­¥éª¤ 2/3: è·³è¿‡æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆ--skip-plotsï¼‰")

        # æ­¥éª¤3ï¼šè¿è¡Œå®Œæ•´ç®¡é“
        logger.info("æ­¥éª¤ 3/3: è¿è¡Œå®Œæ•´æ•°æ®å‡†å¤‡ç®¡é“")
        pipeline.run_complete_pipeline()

        logger.info("âœ… æ•°æ®å‡†å¤‡é˜¶æ®µå®Œæˆï¼")
        logger.info(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")

    except Exception as e:
        logger.error(f"âŒ æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥: {str(e)}")
        raise


def run_data_preprocessing(args: argparse.Namespace) -> None:
    """
    è¿è¡Œæ•°æ®é¢„å¤„ç†é˜¶æ®µ

    Args:
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°
    """
    logger = logging.getLogger(__name__)
    logger.info("=" * 60)
    logger.info("å¼€å§‹é˜¶æ®µäºŒï¼šæ•°æ®é¢„å¤„ç† (Data Preprocessing)")
    logger.info("=" * 60)

    logger.info("ğŸš§ æ•°æ®é¢„å¤„ç†é˜¶æ®µå¾…å®ç°")
    logger.info("å°†åŒ…å«ä»¥ä¸‹æ­¥éª¤:")
    logger.info("1. ç¼ºå¤±å€¼å¤„ç†")
    logger.info("2. å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†")
    logger.info("3. åˆ†ç±»å˜é‡ç¼–ç ")
    logger.info("4. ç‰¹å¾ç¼©æ”¾å’Œæ ‡å‡†åŒ–")
    logger.info("5. ç‰¹å¾å·¥ç¨‹")
    logger.info("6. æ•°æ®é›†åˆ’åˆ†ï¼ˆKæŠ˜äº¤å‰éªŒè¯ï¼‰")


def run_model_building(args: argparse.Namespace) -> None:
    """
    è¿è¡Œæ¨¡å‹æ­å»ºé˜¶æ®µ

    Args:
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°
    """
    logger = logging.getLogger(__name__)
    logger.info("=" * 60)
    logger.info("å¼€å§‹é˜¶æ®µä¸‰ï¼šæ¨¡å‹æ­å»º (Model Building)")
    logger.info("=" * 60)

    logger.info("ğŸš§ æ¨¡å‹æ­å»ºé˜¶æ®µå¾…å®ç°")
    logger.info("å°†åŒ…å«ä»¥ä¸‹æ­¥éª¤:")
    logger.info("1. çº¿æ€§å›å½’æ¨¡å‹")
    logger.info("2. æ­£åˆ™åŒ–å›å½’æ¨¡å‹ï¼ˆRidge/Lasso/Elastic Netï¼‰")
    logger.info("3. æ¨¡å‹é…ç½®å’Œè¶…å‚æ•°è®¾ç½®")


def run_model_training(args: argparse.Namespace) -> None:
    """
    è¿è¡Œæ¨¡å‹è®­ç»ƒé˜¶æ®µ

    Args:
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°
    """
    logger = logging.getLogger(__name__)
    logger.info("=" * 60)
    logger.info("å¼€å§‹é˜¶æ®µå››ï¼šæ¨¡å‹è®­ç»ƒ (Model Training)")
    logger.info("=" * 60)

    logger.info("ğŸš§ æ¨¡å‹è®­ç»ƒé˜¶æ®µå¾…å®ç°")
    logger.info("å°†åŒ…å«ä»¥ä¸‹æ­¥éª¤:")
    logger.info("1. æ¨¡å‹è®­ç»ƒæµç¨‹")
    logger.info("2. äº¤å‰éªŒè¯å®æ–½")
    logger.info("3. è®­ç»ƒè¿‡ç¨‹ç›‘æ§")
    logger.info("4. åˆæ­¥æ€§èƒ½è¯„ä¼°")


def run_model_optimization(args: argparse.Namespace) -> None:
    """
    è¿è¡Œæ¨¡å‹ä¼˜åŒ–é˜¶æ®µ

    Args:
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°
    """
    logger = logging.getLogger(__name__)
    logger.info("=" * 60)
    logger.info("å¼€å§‹é˜¶æ®µäº”ï¼šæ¨¡å‹ä¼˜åŒ– (Model Optimization)")
    logger.info("=" * 60)

    logger.info("ğŸš§ æ¨¡å‹ä¼˜åŒ–é˜¶æ®µå¾…å®ç°")
    logger.info("å°†åŒ…å«ä»¥ä¸‹æ­¥éª¤:")
    logger.info("1. è¶…å‚æ•°è°ƒä¼˜")
    logger.info("2. æ¨¡å‹é›†æˆ")
    logger.info("3. ç‰¹å¾é€‰æ‹©ä¼˜åŒ–")
    logger.info("4. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")


def run_model_evaluation(args: argparse.Namespace) -> None:
    """
    è¿è¡Œæ¨¡å‹è¯„ä¼°é˜¶æ®µ

    Args:
        args (argparse.Namespace): å‘½ä»¤è¡Œå‚æ•°
    """
    logger = logging.getLogger(__name__)
    logger.info("=" * 60)
    logger.info("å¼€å§‹é˜¶æ®µå…­ï¼šæ¨¡å‹è¯„ä¼° (Model Evaluation)")
    logger.info("=" * 60)

    logger.info("ğŸš§ æ¨¡å‹è¯„ä¼°é˜¶æ®µå¾…å®ç°")
    logger.info("å°†åŒ…å«ä»¥ä¸‹æ­¥éª¤:")
    logger.info("1. æœ€ç»ˆæ¨¡å‹æ€§èƒ½è¯„ä¼°")
    logger.info("2. ç‰¹å¾é‡è¦æ€§åˆ†æ")
    logger.info("3. æ¨¡å‹è§£é‡Šæ€§åˆ†æ")
    logger.info("4. å®éªŒæ€»ç»“æŠ¥å‘Š")


def quick_data_check(data_dir: str) -> None:
    """
    å¿«é€Ÿæ•°æ®æ£€æŸ¥

    Args:
        data_dir (str): æ•°æ®ç›®å½•è·¯å¾„
    """
    logger = logging.getLogger(__name__)
    logger.info("ğŸ” æ‰§è¡Œå¿«é€Ÿæ•°æ®æ£€æŸ¥...")

    try:
        # è·å–æ•°æ®æ¦‚è§ˆ
        overview = quick_data_overview(data_dir)

        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        basic_info = overview['basic_info']
        logger.info(f"æ•°æ®å½¢çŠ¶: {basic_info['shape']}")
        logger.info(f"ç‰¹å¾æ•°é‡: {len(basic_info['columns'])}")
        logger.info(f"æ€»ç¼ºå¤±å€¼: {basic_info['total_nulls']}")
        logger.info(f"å†…å­˜ä½¿ç”¨: {basic_info['memory_usage'] / 1024 / 1024:.2f} MB")

        # æ˜¾ç¤ºç›®æ ‡å˜é‡ä¿¡æ¯
        target_info = overview['target_info']
        logger.info(f"ç›®æ ‡å˜é‡èŒƒå›´: {target_info['statistics']['min']:.2f} - {target_info['statistics']['max']:.2f}")
        logger.info(f"ç›®æ ‡å˜é‡å‡å€¼: {target_info['statistics']['mean']:.2f}")

        # æ˜¾ç¤ºæ•°æ®è´¨é‡ä¿¡æ¯
        quality_report = overview['quality_report']
        logger.info(f"é‡å¤æ•°æ®: {quality_report['duplicates']['total_duplicates']}")

        logger.info("âœ… æ•°æ®æ£€æŸ¥å®Œæˆ")

    except Exception as e:
        logger.error(f"âŒ æ•°æ®æ£€æŸ¥å¤±è´¥: {str(e)}")
        raise


def main() -> None:
    """
    ä¸»å‡½æ•°
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # è®¾ç½®æ—¥å¿—
    log_file = args.log_file or f"{args.output_dir}/experiment.log"
    setup_logging(args.log_level, log_file)

    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ å¯åŠ¨åŠ å·æˆ¿ä»·å›å½’å®éªŒ")
    logger.info(f"å®éªŒé˜¶æ®µ: {args.phase}")
    logger.info(f"æ•°æ®ç›®å½•: {args.data_dir}")
    logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"KæŠ˜äº¤å‰éªŒè¯: {args.k_folds}")

    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)

        # æ‰§è¡Œå¿«é€Ÿæ•°æ®æ£€æŸ¥
        if Path(args.data_dir).exists():
            quick_data_check(args.data_dir)
        else:
            logger.warning(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")

        # æ ¹æ®æŒ‡å®šé˜¶æ®µæ‰§è¡Œç›¸åº”åŠŸèƒ½
        if args.phase == "preparation":
            run_data_preparation(args)
        elif args.phase == "preprocessing":
            run_data_preprocessing(args)
        elif args.phase == "building":
            run_model_building(args)
        elif args.phase == "training":
            run_model_training(args)
        elif args.phase == "optimization":
            run_model_optimization(args)
        elif args.phase == "evaluation":
            run_model_evaluation(args)
        elif args.phase == "all":
            run_data_preparation(args)
            run_data_preprocessing(args)
            run_model_building(args)
            run_model_training(args)
            run_model_optimization(args)
            run_model_evaluation(args)

        logger.info("ğŸ‰ å®éªŒå®Œæˆï¼")

    except KeyboardInterrupt:
        logger.info("âš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()